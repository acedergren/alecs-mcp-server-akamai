[
  {
    "title": "Specification - Model Context Protocol",
    "content": "Model Context Protocol home pageVersion 2025-06-18 (latest)Search...⌘KGitHubSearch...NavigationProtocolSpecificationUser GuideIntroductionQuickstartConceptsExamplesTutorialsFAQsProtocolSpecificationKey ChangesArchitectureBase ProtocolClient FeaturesServer FeaturesDevelopmentVersioningRoadmapContributingSDKsC# SDKJava SDKKotlin SDKPython SDKRuby SDKSwift SDKTypeScript SDKProtocolSpecificationCopy page\nModel Context Protocol (MCP) is an open protocol that\nenables seamless integration between LLM applications and external data sources and\ntools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating\ncustom AI workflows, MCP provides a standardized way to connect LLMs with the context\nthey need.\nThis specification defines the authoritative protocol requirements, based on the\nTypeScript schema in\nschema.ts.\nFor implementation guides and examples, visit\nmodelcontextprotocol.io.\nThe key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD\nNOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be\ninterpreted as described in BCP 14\n[RFC2119]\n[RFC8174] when, and only when, they\nappear in all capitals, as shown here.\n​Overview\nMCP provides a standardized way for applications to:\n\nShare contextual information with language models\nExpose tools and capabilities to AI systems\nBuild composable integrations and workflows\n\nThe protocol uses JSON-RPC 2.0 messages to establish\ncommunication between:\n\nHosts: LLM applications that initiate connections\nClients: Connectors within the host application\nServers: Services that provide context and capabilities\n\nMCP takes some inspiration from the\nLanguage Server Protocol, which\nstandardizes how to add support for programming languages across a whole ecosystem of\ndevelopment tools. In a similar way, MCP standardizes how to integrate additional context\nand tools into the ecosystem of AI applications.\n​Key Details\n​Base Protocol\n\nJSON-RPC message format\nStateful connections\nServer and client capability negotiation\n\n​Features\nServers offer any of the following features to clients:\n\nResources: Context and data, for the user or the AI model to use\nPrompts: Templated messages and workflows for users\nTools: Functions for the AI model to execute\n\nClients may offer the following features to servers:\n\nSampling: Server-initiated agentic behaviors and recursive LLM interactions\nRoots: Server-initiated inquiries into uri or filesystem boundaries to operate in\nElicitation: Server-initiated requests for additional information from users\n\n​Additional Utilities\n\nConfiguration\nProgress tracking\nCancellation\nError reporting\nLogging\n\n​Security and Trust & Safety\nThe Model Context Protocol enables powerful capabilities through arbitrary data access\nand code execution paths. With this power comes important security and trust\nconsiderations that all implementors must carefully address.\n​Key Principles\n\n\nUser Consent and Control\n\nUsers must explicitly consent to and understand all data access and operations\nUsers must retain control over what data is shared and what actions are taken\nImplementors should provide clear UIs for reviewing and authorizing activities\n\n\n\nData Privacy\n\nHosts must obtain explicit user consent before exposing user data to servers\nHosts must not transmit resource data elsewhere without user consent\nUser data should be protected with appropriate access controls\n\n\n\nTool Safety\n\nTools represent arbitrary code execution and must be treated with appropriate\ncaution.\n\nIn particular, descriptions of tool behavior such as annotations should be\nconsidered untrusted, unless obtained from a trusted server.\n\n\nHosts must obtain explicit user consent before invoking any tool\nUsers should understand what each tool does before authorizing its use\n\n\n\nLLM Sampling Controls\n\nUsers must explicitly approve any LLM sampling requests\nUsers should control:\n\nWhether sampling occurs at all\nThe actual prompt that will be sent\nWhat results the server can see\n\n\nThe protocol intentionally limits server visibility into prompts\n\n\n\n​Implementation Guidelines\nWhile MCP itself cannot enforce these security principles at the protocol level,\nimplementors SHOULD:\n\nBuild robust consent and authorization flows into their applications\nProvide clear documentation of security implications\nImplement appropriate access controls and data protections\nFollow security best practices in their integrations\nConsider privacy implications in their feature designs\n\n​Learn More\nExplore the detailed specification for each protocol component:\nArchitectureBase ProtocolServer FeaturesClient FeaturesContributingWas this page helpful?YesNoFAQsKey ChangesgithubOn this pageOverviewKey DetailsBase ProtocolFeaturesAdditional UtilitiesSecurity and Trust & SafetyKey PrinciplesImplementation GuidelinesLearn MoreAssistantResponses are generated using AI and may contain mistakes. - GitHub\n\n\n## User Guide\n\n- Introduction\n- Quickstart\n- Concepts\n- Examples\n- Tutorials\n- FAQs\n\n\n## Protocol\n\n- Specification\n- Key Changes\n- Architecture\n- Base Protocol\n- Client Features\n- Server Features\n\n\n## Development\n\n- Versioning\n- Roadmap\n- Contributing\n\n\n## SDKs\n\n- C# SDK\n- Java SDK\n- Kotlin SDK\n- Python SDK\n- Ruby SDK\n- Swift SDK\n- TypeScript SDK\n\n\n## Specification\n\nModel Context Protocol (MCP) is an open protocol that\nenables seamless integration between LLM applications and external data sources and\ntools. Whether you’re building an AI-powered IDE, enhancing a chat interface, or creating\ncustom AI workflows, MCP provides a standardized way to connect LLMs with the context\nthey need.\n\nThis specification defines the authoritative protocol requirements, based on the\nTypeScript schema in\nschema.ts.\n\nFor implementation guides and examples, visit\nmodelcontextprotocol.io.\n\nThe key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD\nNOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be\ninterpreted as described in BCP 14\n[RFC2119]\n[RFC8174] when, and only when, they\nappear in all capitals, as shown here.\n\n\n\n## ​Overview\n\nMCP provides a standardized way for applications to:\n\n- Share contextual information with language models\n- Expose tools and capabilities to AI systems\n- Build composable integrations and workflows\nThe protocol uses JSON-RPC 2.0 messages to establish\ncommunication between:\n\n- Hosts: LLM applications that initiate connections\n- Clients: Connectors within the host application\n- Servers: Services that provide context and capabilities\nMCP takes some inspiration from the\nLanguage Server Protocol, which\nstandardizes how to add support for programming languages across a whole ecosystem of\ndevelopment tools. In a similar way, MCP standardizes how to integrate additional context\nand tools into the ecosystem of AI applications.\n\n\n\n## ​Key Details\n\n\n\n## ​Base Protocol\n\n- JSON-RPC message format\n- Stateful connections\n- Server and client capability negotiation\n\n\n## ​Features\n\nServers offer any of the following features to clients:\n\n- Resources: Context and data, for the user or the AI model to use\n- Prompts: Templated messages and workflows for users\n- Tools: Functions for the AI model to execute\nClients may offer the following features to servers:\n\n- Sampling: Server-initiated agentic behaviors and recursive LLM interactions\n- Roots: Server-initiated inquiries into uri or filesystem boundaries to operate in\n- Elicitation: Server-initiated requests for additional information from users\n\n\n## ​Additional Utilities\n\n- Configuration\n- Progress tracking\n- Cancellation\n- Error reporting\n- Logging\n\n\n## ​Security and Trust & Safety\n\nThe Model Context Protocol enables powerful capabilities through arbitrary data access\nand code execution paths. With this power comes important security and trust\nconsiderations that all implementors must carefully address.\n\n\n\n## ​Key Principles\n\n- User Consent and Control\n\nUsers must explicitly consent to and understand all data access and operations\nUsers must retain control over what data is shared and what actions are taken\nImplementors should provide clear UIs for reviewing and authorizing activities\nUser Consent and Control\n\n- Users must explicitly consent to and understand all data access and operations\n- Users must retain control over what data is shared and what actions are taken\n- Implementors should provide clear UIs for reviewing and authorizing activities\n- Data Privacy\n\nHosts must obtain explicit user consent before exposing user data to servers\nHosts must not transmit resource data elsewhere without user consent\nUser data should be protected with appropriate access controls\nData Privacy\n\n- Hosts must obtain explicit user consent before exposing user data to servers\n- Hosts must not transmit resource data elsewhere without user consent\n- User data should be protected with appropriate access controls\n- Tool Safety\n\nTools represent arbitrary code execution and must be treated with appropriate\ncaution.\n\nIn particular, descriptions of tool behavior such as annotations should be\nconsidered untrusted, unless obtained from a trusted server.\n\n\nHosts must obtain explicit user consent before invoking any tool\nUsers should understand what each tool does before authorizing its use\nTool Safety\n\n- Tools represent arbitrary code execution and must be treated with appropriate\ncaution.\n\nIn particular, descriptions of tool behavior such as annotations should be\nconsidered untrusted, unless obtained from a trusted server.\n- In particular, descriptions of tool behavior such as annotations should be\nconsidered untrusted, unless obtained from a trusted server.\n- Hosts must obtain explicit user consent before invoking any tool\n- Users should understand what each tool does before authorizing its use\n- LLM Sampling Controls\n\nUsers must explicitly approve any LLM sampling requests\nUsers should control:\n\nWhether sampling occurs at all\nThe actual prompt that will be sent\nWhat results the server can see\n\n\nThe protocol intentionally limits server visibility into prompts\nLLM Sampling Controls\n\n- Users must explicitly approve any LLM sampling requests\n- Users should control:\n\nWhether sampling occurs at all\nThe actual prompt that will be sent\nWhat results the server can see\n- Whether sampling occurs at all\n- The actual prompt that will be sent\n- What results the server can see\n- The protocol intentionally limits server visibility into prompts\n\n\n## ​Implementation Guidelines\n\nWhile MCP itself cannot enforce these security principles at the protocol level,\nimplementors SHOULD:\n\n- Build robust consent and authorization flows into their applications\n- Provide clear documentation of security implications\n- Implement appropriate access controls and data protections\n- Follow security best practices in their integrations\n- Consider privacy implications in their feature designs\n\n\n## ​Learn More\n\nExplore the detailed specification for each protocol component:\n\n\n\n## Architecture\n\n\n\n## Base Protocol\n\n\n\n## Server Features\n\n\n\n## Client Features\n\n\n\n## Contributing\n\nWas this page helpful?\n\n- Overview\n- Key Details\n- Base Protocol\n- Features\n- Additional Utilities\n- Security and Trust & Safety\n- Key Principles\n- Implementation Guidelines\n- Learn More\n",
    "url": "https://modelcontextprotocol.io/specification/2025-06-18"
  }
]